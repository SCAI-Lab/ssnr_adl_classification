{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36feec8c",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17da9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading the tabular data'''\n",
    "import pickle\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_pickle_from_parts(parts_dir):\n",
    "    # Combine the parts into a single compressed file\n",
    "    combined_path = os.path.join(parts_dir, 'features_20s.gz')\n",
    "    with open(combined_path, 'wb') as combined_file:\n",
    "        part_num = 0\n",
    "        while True:\n",
    "            part_path = os.path.join(parts_dir, f'features_20s_part_{part_num:03d}')\n",
    "            if not os.path.exists(part_path):\n",
    "                break\n",
    "            with open(part_path, 'rb') as part_file:\n",
    "                shutil.copyfileobj(part_file, combined_file)\n",
    "            part_num += 1\n",
    "    \n",
    "    # Decompress the combined file and load the pickle data\n",
    "    with gzip.open(combined_path, 'rb') as f_in:\n",
    "        data = pickle.load(f_in)\n",
    "    \n",
    "    # Optionally remove the combined file after loading\n",
    "    os.remove(combined_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "current_dir = os.getcwd()  # Use the current working directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "parts_dir = os.path.join(parent_dir, 'data')\n",
    "data_dict = load_pickle_from_parts(parts_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769e6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class | Activity         | Count\n",
      "--------------------------------\n",
      "0     | calmness         | 3646 \n",
      "1     | selfpropulsion   | 1760 \n",
      "2     | armraises        | 1022 \n",
      "3     | transfer         | 2018 \n",
      "4     | usingphone       | 1786 \n",
      "5     | talking          | 2524 \n",
      "6     | washhands        | 2246 \n",
      "7     | eating           | 1618 \n",
      "8     | assistedprop     | 1534 \n",
      "9     | usingcomputer    | 2494 \n",
      "10    | changingclothes  | 1126 \n",
      "11    | pressurerelief   | 1398 \n"
     ]
    }
   ],
   "source": [
    "'''Loading the data'''\n",
    "\n",
    "# Name of the devices to include in the pipeline\n",
    "devices = [\n",
    "    'corsano_wrist',\n",
    "    'cosinuss_ear',\n",
    "    'sensomative_back',\n",
    "    'sensomative_bottom',\n",
    "    'vivalink_patch',\n",
    "    'zurichmove_wheel'\n",
    "]\n",
    "n_devices = len(devices)\n",
    "\n",
    "# Load parameters from the yaml file\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()  # Use the current working directory\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "# Construct the path to the yaml file\n",
    "yaml_file_path = os.path.join(parent_dir, 'parameters.yaml')\n",
    "# Load the yaml file\n",
    "with open(yaml_file_path, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "# Accessing the parameters\n",
    "seed_number = params['seed_number']\n",
    "upsample_freq = params['upsample_freq']\n",
    "activities_label_mapping = params['activities_label_mapping']\n",
    "\n",
    "'''Loading the data'''\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='overflow encountered in cast')\n",
    "\n",
    "\n",
    "# Converting the dict data to a dataframe\n",
    "dfs = []\n",
    "subjects = []\n",
    "for i_subject, subject in enumerate(data_dict.keys()):\n",
    "    df1 = data_dict[subject]['corsano_wrist']\n",
    "    df2 = data_dict[subject]['cosinuss_ear']\n",
    "    df3 = data_dict[subject]['sensomative_back']\n",
    "    df4 = data_dict[subject]['sensomative_bottom']\n",
    "    df5 = data_dict[subject]['vivalink_patch']\n",
    "    df6 = data_dict[subject]['zurichmove_wheel']\n",
    "    df7 = data_dict[subject]['label']\n",
    "    df = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=1)\n",
    "    df['subject'] = i_subject\n",
    "    subjects.append(subject)\n",
    "    dfs.append(df)\n",
    "data_df = pd.concat(dfs)\n",
    "\n",
    "'''Converting the dataframe to float32 except for the 'label' and 'subject' columns'''\n",
    "data_df = data_df.astype('float32')\n",
    "data_df[['label', 'subject']] = data_df[['label', 'subject']].astype(int)\n",
    "\n",
    "'''Drop columns containing inf and nan values'''\n",
    "data_df = data_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1, how='any')\n",
    "\n",
    "'''Updating the list of features for each device'''\n",
    "device_columns = []\n",
    "for device in devices:\n",
    "    columns = []\n",
    "    for column in data_df.columns:\n",
    "        if device in column:\n",
    "            columns.append(column)\n",
    "    device_columns.append(columns)\n",
    "\n",
    "# Convert the pandas df to cudf\n",
    "# data_df = cudf.DataFrame.from_pandas(data_df)\n",
    "\n",
    "# Reporting the data imbalance\n",
    "X = np.array(data_df.drop(['label', 'subject'], axis=1, inplace=False))\n",
    "y = np.array(data_df['label'])\n",
    "subjects = np.array(data_df['subject'])\n",
    "# Get unique classes and their counts\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "# Create a report table\n",
    "report_table = np.vstack((classes, list(activities_label_mapping.keys()), counts)).T\n",
    "print(\"Class | Activity         | Count\")\n",
    "print(\"--------------------------------\")\n",
    "for row in report_table:\n",
    "    print(f\"{row[0]:<5} | {row[1]:<16} | {row[2]:<5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd4261",
   "metadata": {},
   "source": [
    "### Performing nested cross validation (cv splits done by subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55872e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "# Custom transformer for clipping\n",
    "class ClippingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)\n",
    "\n",
    "# Define the subject-based k-fold splitting function\n",
    "def subject_kfold(subjects, subject_col, n_test_subjects):\n",
    "    splits = []\n",
    "    num_folds = len(subjects) // n_test_subjects\n",
    "    test_folds = []\n",
    "    subjects_copy = subjects.copy()\n",
    "    for i in range(num_folds):\n",
    "        test_fold = random.sample(subjects_copy, n_test_subjects)\n",
    "        test_folds.append(test_fold)\n",
    "        subjects_copy = [elem for elem in subjects_copy if elem not in test_fold]\n",
    "    test_folds[-1].extend(subjects_copy)\n",
    "    for test_fold in test_folds:\n",
    "        train_fold = [elem for elem in subjects if elem not in test_fold]\n",
    "        test_indices = np.where(np.isin(subject_col, test_fold))[0]\n",
    "        train_indices = np.where(np.isin(subject_col, train_fold))[0]\n",
    "        splits.append((train_indices, test_indices))\n",
    "    return splits, test_folds\n",
    "\n",
    "# Assuming your DataFrame is named 'data_df' and it has columns 'subject', 'label', and other features\n",
    "# Separate features, labels, and groups (subjects)\n",
    "X = data_df.drop(columns=['subject', 'label'])\n",
    "y = data_df['label']\n",
    "subject_col = data_df['subject']\n",
    "subjects = list(set(subject_col))\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Number of subjects to be used in the test and valdation sets for each fold\n",
    "n_test_subjects = len(subjects) // 5\n",
    "n_val_subjects = (len(subjects) - n_test_subjects) // 5\n",
    "\n",
    "# Define the pipeline (GPU)\n",
    "pipeline = Pipeline([\n",
    "    ('remove_constant', VarianceThreshold(threshold=0.0)),\n",
    "    ('clipper', ClippingTransformer(lower_percentile=1, upper_percentile=99)),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('selector', SelectKBest(score_func=mutual_info_classif)),\n",
    "    ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', tree_method = \"hist\", device = \"cuda\"))\n",
    "])\n",
    "\n",
    "# # Define the pipeline (CPU)\n",
    "# pipeline = Pipeline([\n",
    "#     ('remove_constant', VarianceThreshold(threshold=0.0)),\n",
    "#     ('clipper', ClippingTransformer(lower_percentile=1, upper_percentile=99)),\n",
    "#     ('scaler', RobustScaler()),\n",
    "#     ('selector', SelectKBest(score_func=mutual_info_classif)),\n",
    "#     ('classifier', XGBClassifier(random_state=42, xgb_booster='gbtree'))\n",
    "# ])\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'selector__k': [500],\n",
    "    'classifier__n_estimators': [150],\n",
    "    'classifier__max_depth': [10],\n",
    "    'classifier__learning_rate': [0.01],\n",
    "    'classifier__subsample': [0.8],\n",
    "    'classifier__colsample_bytree': [1.0],\n",
    "    'classifier__gamma': [0.3]\n",
    "}\n",
    "\n",
    "outer_scores = []\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Perform nested cross-validation using subject-based splits\n",
    "cv_outer, test_folds = subject_kfold(subjects, subject_col, n_test_subjects)\n",
    "\n",
    "for i_test_fold in range(len(test_folds)):\n",
    "    train_ix, test_ix = cv_outer[i_test_fold]\n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    subject_col_train = subject_col.iloc[train_ix]\n",
    "    train_subjects = [elem for elem in subjects if elem not in test_folds[i_test_fold]]\n",
    "\n",
    "    # Inner cross-validation\n",
    "    cv_inner, valid_folds = subject_kfold(train_subjects, subject_col_train, n_val_subjects)\n",
    "    inner_cv_splits = [(train_inner_ix, val_inner_ix) for train_inner_ix, val_inner_ix in cv_inner]\n",
    "\n",
    "    # RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=inner_cv_splits, n_iter=1, random_state=42, refit=True, scoring='f1_micro', n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the best model found by RandomizedSearchCV on the test set\n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Store results\n",
    "    all_predictions.extend(y_pred)\n",
    "    all_true_labels.extend(y_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "precision = precision_score(all_true_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "balanced_acc = balanced_accuracy_score(all_true_labels, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=range(len(conf_matrix)), columns=range(len(conf_matrix)))\n",
    "class_report = classification_report(all_true_labels, all_predictions)\n",
    "report = classification_report(all_true_labels, all_predictions, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Display metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'Balanced Accuracy'],\n",
    "    'Value': [accuracy, precision, recall, f1, balanced_acc]\n",
    "})\n",
    "\n",
    "# Print metrics\n",
    "print(\"Classification Metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot classification metrics\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "report_df = report_df[:-3]  # Remove the last three rows: 'accuracy', 'macro avg', 'weighted avg'\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    sns.barplot(x=report_df.index, y=report_df[metric], ax=ax[idx])\n",
    "    ax[idx].set_title(f'{metric.capitalize()} by Class')\n",
    "    ax[idx].set_xlabel('Class')\n",
    "    ax[idx].set_ylabel(metric.capitalize())\n",
    "    ax[idx].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssnr_2024",
   "language": "python",
   "name": "ssnr_2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
